{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"TbhoqHAOth4s"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset, random_split\n","from sklearn.metrics import r2_score\n","import random\n","from scipy.signal import savgol_filter\n","from sklearn.metrics import mean_squared_error\n","import os"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w-h5tlwDvLZ7"},"outputs":[],"source":["# Function to estimate log-normal distribution parameters from mean and median\n","def estimate_lognormal_params(mean, median):\n","    sigma = np.sqrt(2 * np.log(mean / median))\n","    mu = np.log(median)\n","    return mu, sigma\n","\n","# Function to generate synthetic data using log-normal distribution and clip to observed range\n","def generate_lognormal_samples(mean, median, min_val, max_val, size=10000):\n","    mu, sigma = estimate_lognormal_params(mean, median)\n","    samples = np.random.lognormal(mean=mu, sigma=sigma, size=size)\n","    return np.clip(samples, min_val, max_val)\n","\n","\n","def generate_time (t_initial, t_final, n_timesteps):\n","    t_numpy = np.linspace(t_initial, t_final, n_timesteps)\n","    t_reshape = t_numpy.reshape(1, -1)\n","    t_torch = torch.tensor(t_reshape, dtype=torch.float32).to(device)\n","    return t_numpy, t_reshape, t_torch\n","\n","def zscore(x, eps=1e-12):\n","    x = np.asarray(x, dtype=float)\n","    mu = x.mean()\n","    sd = x.std()\n","    if sd < eps:\n","        return x * 0.0\n","    return (x - mu) / sd\n","\n","def best_lag_correlation(a, b, max_lag=None, normalize='zscore'):\n","\n","    a = np.asarray(a, dtype=float)\n","    b = np.asarray(b, dtype=float)\n","\n","    T = min(len(a), len(b))\n","    a = a[:T]\n","    b = b[:T]\n","\n","    if normalize == 'zscore':\n","        a_n = zscore(a)\n","        b_n = zscore(b)\n","    else:\n","        a_n = a\n","        b_n = b\n","\n","    if max_lag is None:\n","        max_lag = T - 1\n","\n","    best_corr = -np.inf\n","    best_lag = 0\n","    best_pair = (None, None)\n","\n","\n","    for lag in range(-max_lag, max_lag + 1):\n","        if lag < 0:\n","\n","            a_slice = a_n[-lag:]\n","            b_slice = b_n[:T+lag]\n","        elif lag > 0:\n","\n","            a_slice = a_n[:T-lag]\n","            b_slice = b_n[lag:]\n","        else:\n","            a_slice = a_n\n","            b_slice = b_n\n","\n","        if len(a_slice) < 2:\n","            continue\n","\n","        corr = np.dot(a_slice, b_slice) / (len(a_slice) - 1)\n","\n","        if corr > best_corr:\n","            best_corr = corr\n","            best_lag = lag\n","\n","            if lag < 0:\n","                a_raw = a[-lag:]\n","                b_raw = b[:T+lag]\n","            elif lag > 0:\n","                a_raw = a[:T-lag]\n","                b_raw = b[lag:]\n","            else:\n","                a_raw = a\n","                b_raw = b\n","            best_pair = (a_raw, b_raw)\n","\n","    return {'lag': best_lag, 'corr': best_corr, 'a_seg': best_pair[0], 'b_seg': best_pair[1]}\n","\n","def smooth_peak(curve, window_length=11, polyorder=3, baseline_ratio=0.01):\n","    curve = np.asarray(curve)\n","    smoothed = savgol_filter(curve, window_length=window_length, polyorder=polyorder)\n","\n","    peak_idx = np.argmax(smoothed)\n","    peak_val = smoothed[peak_idx]\n","    threshold = peak_val * baseline_ratio\n","\n","    # Find start (left of peak)\n","    start_idx = 0\n","    for i in range(peak_idx, 0, -1):\n","        if smoothed[i] < threshold:\n","            start_idx = i\n","            break\n","\n","    # Find end (right of peak)\n","    end_idx = len(curve) - 1\n","    for i in range(peak_idx, len(curve)):\n","        if smoothed[i] < threshold:\n","            end_idx = i\n","            break\n","\n","    # Keep only the main peak region\n","    cleaned = np.zeros_like(curve)\n","    cleaned[start_idx:end_idx + 1] = curve[start_idx:end_idx + 1]\n","\n","    return cleaned"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GDT-ZP9QvQ8D"},"outputs":[],"source":["# Set random seed for reproducibility\n","np.random.seed(42)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0xN4Y3QuvTaY"},"outputs":[],"source":["# Generate synthetic values for each variable\n","mass     = generate_lognormal_samples(mean=9.62,     median=2.81,     min_val=0.0404,    max_val=108.86)\n","area     = generate_lognormal_samples(mean=224.82,   median=17.09,    min_val=0.98,      max_val=4931.81)\n","velocity = generate_lognormal_samples(mean=0.51,     median=0.40,     min_val=0.04,      max_val=1.58)\n","distance = generate_lognormal_samples(mean=46162.12, median=29611.86, min_val=35.41,     max_val=294509.22)\n","dispersion = generate_lognormal_samples(mean=130.73,     median=37.48,     min_val=1.9,      max_val=1486.45)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mR4NC_lkvVgO"},"outputs":[],"source":["# Linear scale and reshape\n","M = mass.reshape(-1,1)\n","A = area.reshape(-1,1)\n","x = distance.reshape(-1,1)\n","u = velocity.reshape(-1,1)\n","Q = np.array(A*u)\n","Q = Q.reshape(-1,1)\n","R= 1\n","theta = 1\n","D = dispersion.reshape(-1,1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OHlF086wvW3c"},"outputs":[],"source":["n_timesteps = 100000\n","t_numpy, t_reshape, t_torch = generate_time(1, 3e6, n_timesteps)\n","c_downstream_filter = (1e6 * M) / (2 * theta * A * R * np.sqrt(np.pi * D * t_reshape/ R)) * np.exp(-((x - u * t_reshape / R) ** 2) / (4 * D * t_reshape / R))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TF3KE04KvYnO"},"outputs":[],"source":["t_peak = []\n","max_concentrion_array = []\n","for i in range (c_downstream_filter.shape[0]):\n","  max_concentrion_idx = np.argmax(c_downstream_filter[i,:])\n","  max_concentrion = np.max(c_downstream_filter[i,:])\n","  time_peak = t_reshape[:,max_concentrion_idx]\n","  t_peak = np.append(t_peak,time_peak)\n","  max_concentrion_array = np.append(max_concentrion_array,max_concentrion)\n","\n","v = distance/t_peak\n","pe = (distance*velocity)/dispersion\n","valid_v = (v >= 0.1) & (v <= 6)\n","valid_pe = (pe >= 10) & (pe <= 1500)\n","valid_max_c = (max_concentrion_array >= 0.3) & (max_concentrion_array <= 3000)\n","valid_ratio =  valid_v & valid_pe & valid_max_c"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3uUFjfqevaDr"},"outputs":[],"source":["# Linear scale and reshape\n","valid_M = M[valid_ratio]\n","valid_M = valid_M[0:3000]\n","valid_A = A[valid_ratio]\n","valid_A = valid_A[0:3000]\n","valid_x = x[valid_ratio]\n","valid_x = valid_x[0:3000]\n","x_upstream = np.array(valid_x/ 10)\n","x_upstream = x_upstream.reshape(-1,1)\n","valid_u = u[valid_ratio]\n","valid_u = valid_u[0:3000]\n","valid_D = D[valid_ratio]\n","valid_D = valid_D[0:3000]\n","valid_Q = np.array(valid_A*valid_u)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2oCrVfLAvbXf"},"outputs":[],"source":["# Calculate upstream and downstream BTCs using ADE analytical solution\n","c_upstream = (1e6 * valid_M) / (2 * theta * valid_A * R * np.sqrt(np.pi * valid_D * t_reshape / R)) * np.exp(-((x_upstream - valid_u * t_reshape / R) ** 2) / (4 * valid_D * t_reshape / R))\n","\n","c_downstream = (1e6 * valid_M) / (2 * theta * valid_A * R * np.sqrt(np.pi * valid_D * t_reshape/ R)) * np.exp(-((valid_x - valid_u * t_reshape / R) ** 2) / (4 * valid_D * t_reshape / R))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VrfwRY2svc2r"},"outputs":[],"source":["X = torch.tensor(c_upstream,dtype=torch.float32).to(device)\n","y = torch.tensor(c_downstream,dtype=torch.float32).to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N7tgwsZSvm5j"},"outputs":[],"source":["# Define the MLP model\n","class MLP(nn.Module):\n","    def __init__(self, input_size, output_size):\n","        super(MLP, self).__init__()\n","        self.model = nn.Sequential(\n","            nn.Linear(input_size, 2048),\n","            nn.ReLU(),\n","            nn.Linear(2048, 1024),\n","            nn.ReLU(),\n","            nn.Linear(1024, output_size)\n","        )\n","\n","    def forward(self, x):\n","        return self.model(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D0zjTGTEn-3_","outputId":"9ae227d6-d5b0-4525-d4af-9b014f111337"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Training with 100 samples (5 repetitions)...\n","Finished 100 samples: Avg Validation Loss = 423.709606, Avg Peak R² = 0.798980, Avg Tpeak R² = 0.938410, Avg M0 R² = 0.593927, Avg CC = 0.863300\n","peak mape = 32.08690643310547, peak bias = 0.7062751650810242\n","tpeak mape = 10.974352936932345, tpeak bias = 1.0012189204481756)\n","m0 mape = 23.504820955321545, m0 bias = 1.0479491698637091\n","m1 mape = 24.064758290179263, m1 bias = 1.0014431566137136\n","\n","Training with 200 samples (5 repetitions)...\n","Finished 200 samples: Avg Validation Loss = 511.072139, Avg Peak R² = 0.854124, Avg Tpeak R² = 0.947570, Avg M0 R² = 0.967603, Avg CC = 0.877541\n","peak mape = 20.3961238861084, peak bias = 0.835231602191925\n","tpeak mape = 8.423939415986817, tpeak bias = 0.9525974168658233)\n","m0 mape = 15.254164326770285, m0 bias = 0.9649763247453166\n","m1 mape = 15.983539714380516, m1 bias = 0.9748281823423796\n","\n","Training with 300 samples (5 repetitions)...\n","Finished 300 samples: Avg Validation Loss = 111.652875, Avg Peak R² = 0.859135, Avg Tpeak R² = 0.955300, Avg M0 R² = 0.977893, Avg CC = 0.904894\n","peak mape = 19.05320930480957, peak bias = 0.7765518426895142\n","tpeak mape = 8.797926448737375, tpeak bias = 0.9720375772786758)\n","m0 mape = 13.166097197525705, m0 bias = 1.0334149310142\n","m1 mape = 18.70315696652819, m1 bias = 1.025406481514839\n","\n","Training with 400 samples (5 repetitions)...\n","Finished 400 samples: Avg Validation Loss = 309.243219, Avg Peak R² = 0.884622, Avg Tpeak R² = 0.680044, Avg M0 R² = 0.785276, Avg CC = 0.892772\n","peak mape = 23.644832611083984, peak bias = 0.7785598039627075\n","tpeak mape = 8.637417031670669, tpeak bias = 0.9245023754091347)\n","m0 mape = 12.19511053513008, m0 bias = 0.960113800137659\n","m1 mape = 13.899376794880052, m1 bias = 0.8910318354386433\n","\n","Training with 500 samples (5 repetitions)...\n","Finished 500 samples: Avg Validation Loss = 284.962150, Avg Peak R² = 0.839696, Avg Tpeak R² = 0.770150, Avg M0 R² = 0.952599, Avg CC = 0.904583\n","peak mape = 20.351266860961914, peak bias = 0.7585436701774597\n","tpeak mape = 7.293149586856788, tpeak bias = 0.9094071480988122)\n","m0 mape = 11.817592363969872, m0 bias = 0.9713485331881276\n","m1 mape = 12.931194040323652, m1 bias = 0.8676112011789305\n","\n","Training with 600 samples (5 repetitions)...\n","Finished 600 samples: Avg Validation Loss = 133.893451, Avg Peak R² = 0.850379, Avg Tpeak R² = 0.984070, Avg M0 R² = 0.986495, Avg CC = 0.934979\n","peak mape = 17.975637435913086, peak bias = 0.7905441522598267\n","tpeak mape = 7.730146605921246, tpeak bias = 0.9764924885731798)\n","m0 mape = 8.823529936429503, m0 bias = 1.0146849152036441\n","m1 mape = 9.97113228547386, m1 bias = 0.99422467639857\n","\n","Training with 800 samples (5 repetitions)...\n","Finished 800 samples: Avg Validation Loss = 116.976679, Avg Peak R² = 0.936405, Avg Tpeak R² = 0.885271, Avg M0 R² = 0.982371, Avg CC = 0.944997\n","peak mape = 14.983682632446289, peak bias = 0.8471412658691406\n","tpeak mape = 4.786705263799786, tpeak bias = 0.9650747616402191)\n","m0 mape = 7.764592739961245, m0 bias = 1.0055107257510034\n","m1 mape = 8.699071043820807, m1 bias = 0.969447723564809\n","\n","Training with 1000 samples (5 repetitions)...\n","Finished 1000 samples: Avg Validation Loss = 73.151639, Avg Peak R² = 0.922608, Avg Tpeak R² = 0.967570, Avg M0 R² = 0.987583, Avg CC = 0.946100\n","peak mape = 15.114362716674805, peak bias = 0.8489720225334167\n","tpeak mape = 4.2194452687857105, tpeak bias = 1.0039385370730058)\n","m0 mape = 6.335244634825847, m0 bias = 1.0335832433036867\n","m1 mape = 7.36268984194893, m1 bias = 1.0537774479065867\n","\n","Training with 1200 samples (5 repetitions)...\n","Finished 1200 samples: Avg Validation Loss = 46.845273, Avg Peak R² = 0.941813, Avg Tpeak R² = 0.961950, Avg M0 R² = 0.994361, Avg CC = 0.963541\n","peak mape = 11.876768112182617, peak bias = 0.8935703039169312\n","tpeak mape = 3.4422514926389036, tpeak bias = 0.9735575105616515)\n","m0 mape = 6.0843824836094145, m0 bias = 1.0100278698467697\n","m1 mape = 6.75960291186985, m1 bias = 1.0045962550655265\n","\n","Training with 1500 samples (5 repetitions)...\n","Finished 1500 samples: Avg Validation Loss = 68.350734, Avg Peak R² = 0.906744, Avg Tpeak R² = 0.960799, Avg M0 R² = 0.967358, Avg CC = 0.950307\n","peak mape = 14.465411186218262, peak bias = 0.8387729525566101\n","tpeak mape = 3.518865434419884, tpeak bias = 0.979461000130453)\n","m0 mape = 7.162509882586649, m0 bias = 0.986313904770646\n","m1 mape = 8.015423046212499, m1 bias = 0.8859861438493872\n","\n","Training with 2000 samples (5 repetitions)...\n","Finished 2000 samples: Avg Validation Loss = 27.135758, Avg Peak R² = 0.965395, Avg Tpeak R² = 0.848009, Avg M0 R² = 0.996088, Avg CC = 0.968554\n","peak mape = 10.134790420532227, peak bias = 0.9070974588394165\n","tpeak mape = 3.058488788089188, tpeak bias = 0.9379908434239053)\n","m0 mape = 5.227751177300451, m0 bias = 1.0003515792226612\n","m1 mape = 5.7419525976059145, m1 bias = 0.9584631339281362\n","\n","Training with 3000 samples (5 repetitions)...\n"]}],"source":["subset_sizes = [100, 200, 300, 400, 500, 600, 800, 1000, 1200, 1500, 2000,3000]\n","final_losses_mean = []\n","train_losses_mean = []\n","cc_results_mean = []\n","\n","\n","r2_results_peak_mean = []\n","r2_results_tpeak_mean = []\n","r2_results_m0_mean = []\n","r2_results_m1_mean = []\n","mape_results_peak = []\n","mape_results_tpeak = []\n","mape_results_m0 = []\n","mape_results_m1 = []\n","bias_results_peak = []\n","bias_results_tpeak = []\n","bias_results_m0 = []\n","bias_results_m1 = []\n","\n","\n","# Set the number of repetitions\n","repeats = 5\n","\n","for n_samples in subset_sizes:\n","    print(f\"\\nTraining with {n_samples} samples ({repeats} repetitions)...\")\n","\n","    final_losses_rep = []\n","    train_losses_rep = []\n","    cc_results_rep = []\n","\n","    r2_results_peak_rep = []\n","    r2_results_tpeak_rep = []\n","    r2_results_m0_rep = []\n","    r2_results_m1_rep = []\n","    mape_results_peak_rep = []\n","    mape_results_tpeak_rep = []\n","    mape_results_m0_rep = []\n","    mape_results_m1_rep = []\n","    bias_results_peak_rep = []\n","    bias_results_tpeak_rep = []\n","    bias_results_m0_rep = []\n","    bias_results_m1_rep = []\n","\n","\n","    for rep in range(repeats):\n","        # 1. Take a subset of the data\n","        subset_X = X[:n_samples, :]\n","        subset_y = y[:n_samples, :]\n","\n","        # 2. Create TensorDataset and split\n","        dataset = TensorDataset(subset_X, subset_y)\n","        train_size = int(0.7 * n_samples)\n","        val_size = int(0.2 * n_samples)\n","        test_size = n_samples - train_size - val_size\n","        set_seed(rep)  # Change seed each repetition for diversity\n","        train_set, val_set, test_set = random_split(dataset, [train_size, val_size, test_size])\n","\n","        train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n","        val_loader = DataLoader(val_set, batch_size=32, shuffle=False)\n","        test_loader = DataLoader(test_set, batch_size=32, shuffle=True)\n","\n","        # 3. Initialize new model each time\n","        input_size = subset_X.shape[1]\n","        output_size = subset_y.shape[1]\n","        model = MLP(input_size, output_size).to(device)\n","        criterion = nn.MSELoss()\n","        optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-3)\n","        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=200, gamma=0.5)\n","\n","        # 4. Train model\n","        epochs = 1000\n","        for epoch in range(epochs):\n","            model.train()\n","            running_loss = 0.0\n","            for batch_X, batch_y in train_loader:\n","                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n","                optimizer.zero_grad()\n","                outputs = model(batch_X)\n","                loss = criterion(outputs, batch_y)\n","                loss.backward()\n","                optimizer.step()\n","                running_loss += loss.item()\n","            scheduler.step()\n","\n","        avg_train_loss = running_loss / len(train_loader)\n","        train_losses_rep.append(avg_train_loss)\n","\n","        # 5. Evaluate on validation set\n","        model.eval()\n","        val_loss = 0.0\n","        with torch.no_grad():\n","            for batch_X, batch_y in val_loader:\n","                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n","                outputs = model(batch_X)\n","                loss = criterion(outputs, batch_y)\n","                val_loss += loss.item()\n","        avg_val_loss = val_loss / len(val_loader)\n","        final_losses_rep.append(avg_val_loss)\n","\n","        # 6. Evaluate on test set\n","        model.eval()\n","        actual_curves = []\n","        predicted_curves = []\n","        with torch.no_grad():\n","            for batch_X, batch_y in test_loader:\n","                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n","                outputs = model(batch_X)\n","                actual_curves.extend(batch_y.cpu().numpy())\n","                predicted_curves.extend(outputs.cpu().numpy())\n","\n","        actual_curves = np.array(actual_curves)\n","        predicted_curves = np.array(predicted_curves)\n","        predicted_curves[predicted_curves < 0] = 0\n","\n","\n","\n","        # CROSS CORELATION\n","        max_lag = 100\n","        use_normalize = 'zscore'\n","        cc_results = []\n","        for i in range(actual_curves.shape[0]):\n","            a = actual_curves[i]\n","            b = predicted_curves[i]\n","            res = best_lag_correlation(a, b, max_lag=max_lag, normalize=use_normalize)\n","            cc_results.append(res)\n","        cc_mean_corr = np.mean([r['corr'] for r in cc_results])\n","        cc_results_rep.append(cc_mean_corr)\n","\n","\n","\n","        # Peak concentration\n","        actual_peak = np.max(actual_curves, axis=1)\n","        predicted_peak = np.max(predicted_curves, axis=1)\n","        r2_peak = r2_score(actual_peak, predicted_peak)\n","        r2_results_peak_rep.append(r2_peak)\n","        mape_peak = np.mean(np.abs((predicted_peak - actual_peak) / actual_peak)) * 100\n","        mape_results_peak_rep.append(mape_peak)\n","        bias_peak = np.mean(predicted_peak) / np.mean(actual_peak)\n","        bias_results_peak_rep.append(bias_peak)\n","\n","        # Time to peak\n","        actual_peak_time = t_numpy[np.argmax(actual_curves, axis=1)]\n","        predicted_peak_time = t_numpy[np.argmax(predicted_curves, axis=1)]\n","        r2_tpeak = r2_score(actual_peak_time, predicted_peak_time)\n","        r2_results_tpeak_rep.append(r2_tpeak)\n","        mape_tpeak = np.mean(np.abs((predicted_peak_time - actual_peak_time) / actual_peak_time)) * 100\n","        mape_results_tpeak_rep.append(mape_tpeak)\n","        bias_tpeak = np.mean(predicted_peak_time) / np.mean(actual_peak_time)\n","        bias_results_tpeak_rep.append(bias_tpeak)\n","\n","        #M0\n","        actual_m0 = []\n","        predicted_m0 = []\n","        for i in range(actual_curves.shape[0]):\n","            actual_m0.append(np.trapezoid(actual_curves[i,:],t_reshape))\n","            predicted_m0.append(np.trapezoid(smooth_peak(predicted_curves[i,:]),t_reshape))\n","        actual_m0 = np.array(actual_m0)\n","        predicted_m0 = np.array(predicted_m0)\n","        r2_m0 = r2_score(actual_m0, predicted_m0)\n","        r2_results_m0_rep.append(r2_m0)\n","        mape_m0 = np.mean(np.abs((predicted_m0 - actual_m0) / actual_m0)) * 100\n","        mape_results_m0_rep.append(mape_m0)\n","        bias_m0 = np.mean(predicted_m0) / np.mean(actual_m0)\n","        bias_results_m0_rep.append(bias_m0)\n","\n","        #M1\n","        actual_m1 = []\n","        predicted_m1 = []\n","        for i in range(actual_curves.shape[0]):\n","            actual_moment = actual_curves[i,:]*t_reshape\n","            predicted_moment = smooth_peak(predicted_curves[i,:])*t_reshape\n","            actual_m1.append(np.trapezoid(actual_moment,t_reshape))\n","            predicted_m1.append(np.trapezoid(predicted_moment,t_reshape))\n","        actual_m1 = np.array(actual_m1)\n","        predicted_m1 = np.array(predicted_m1)\n","        r2_m1 = r2_score(actual_m1, predicted_m1)\n","        r2_results_m1_rep.append(r2_m1)\n","        mape_m1 = np.mean(np.abs((predicted_m1 - actual_m1) / actual_m1)) * 100\n","        mape_results_m1_rep.append(mape_m1)\n","        bias_m1 = np.mean(predicted_m1) / np.mean(actual_m1)\n","        bias_results_m1_rep.append(bias_m1)\n","\n","\n","\n","    # After the repetitions, take the mean\n","    final_losses_mean.append(np.mean(final_losses_rep)) # val loss\n","    train_losses_mean.append(np.mean(train_losses_rep)) # trian loss\n","    cc_results_mean.append(np.mean(cc_results_rep)) #cc\n","    r2_results_peak_mean.append(np.mean(r2_results_peak_rep)) # peak r2\n","    r2_results_tpeak_mean.append(np.mean(r2_results_tpeak_rep)) #tpeak r2\n","    r2_results_m0_mean.append(np.mean(r2_results_m0_rep)) #m0 r2\n","    r2_results_m1_mean.append(np.mean(r2_results_m1_rep)) #m1 r2\n","    mape_results_peak.append(np.mean(mape_results_peak_rep)) # mape peak\n","    mape_results_tpeak.append(np.mean(mape_results_tpeak_rep))  # mape tpeak\n","    mape_results_m0.append(np.mean(mape_results_m0_rep)) # mape m0\n","    mape_results_m1.append(np.mean(mape_results_m1_rep))  # mape m1\n","    bias_results_peak.append(np.mean(bias_results_peak_rep)) # bias peak\n","    bias_results_tpeak.append(np.mean(bias_results_tpeak_rep)) # bias tpeak\n","    bias_results_m0.append(np.mean(bias_results_m0_rep)) # bias m0\n","    bias_results_m1.append(np.mean(bias_results_m1_rep)) # bias m1\n","\n","\n","    print(f\"Finished {n_samples} samples: Avg Validation Loss = {np.mean(final_losses_rep):.6f}, Avg Peak R² = {np.mean(r2_results_peak_rep):.6f}, Avg Tpeak R² = {np.mean(r2_results_tpeak_rep):.6f}, Avg M0 R² = {np.mean(r2_results_m0_rep):.6f}, Avg CC = {np.mean(cc_results_rep):.6f}\")\n","    print(f\"peak mape = {np.mean(mape_results_peak_rep)}, peak bias = {np.mean(bias_results_peak_rep)}\")\n","    print(f\"tpeak mape = {np.mean(mape_results_tpeak_rep)}, tpeak bias = {np.mean(bias_results_tpeak_rep)})\")\n","    print(f\"m0 mape = {np.mean(mape_results_m0_rep)}, m0 bias = {np.mean(bias_results_m0_rep)}\")\n","    print(f\"m1 mape = {np.mean(mape_results_m1_rep)}, m1 bias = {np.mean(bias_results_m1_rep)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E1NyscYUh0V8"},"outputs":[],"source":["plt.figure(figsize=(10, 6))\n","plt.plot(subset_sizes, final_losses_mean, marker='o', label='Validation loss')\n","plt.plot(subset_sizes, train_losses_mean, marker='x', label='Train loss')\n","plt.xlabel(\"Number of synthetic breakthrough curves\")\n","plt.ylabel(\"Loss (Final Epoch)\")\n","plt.legend()\n","plt.grid(True)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GwUdxFf5QVoG"},"outputs":[],"source":["plt.figure(figsize=(10, 6))\n","plt.plot(subset_sizes, cc_results_mean, marker='o')\n","plt.xlabel(\"Number of synthetic breakthrough curves\")\n","plt.ylabel(\"Cross-Correlation Score\")\n","plt.grid(True)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pnav4WQv4JD5"},"outputs":[],"source":["plt.figure(figsize=(10, 6))\n","plt.plot(subset_sizes, r2_results_peak_mean, marker='o')\n","plt.xlabel(\"Number of synthetic breakthrough curves\")\n","plt.ylabel(\"R$^2$ for peak concentration\")\n","plt.grid(True)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kAIePyxrQp1q"},"outputs":[],"source":["plt.figure(figsize=(10, 6))\n","plt.plot(subset_sizes, mape_results_peak, marker='o')\n","plt.xlabel(\"Number of synthetic breakthrough curves\")\n","plt.ylabel(\"MAPE for peak concentration (%)\")\n","plt.grid(True)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nrHl8QlSSJpe"},"outputs":[],"source":["plt.figure(figsize=(10, 6))\n","plt.plot(subset_sizes, bias_results_peak, marker='o')\n","plt.xlabel(\"Number of synthetic breakthrough curves\")\n","plt.ylabel(\"Bias ratio for peak concentration\")\n","plt.grid(True)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_SvAcYGbPbC2"},"outputs":[],"source":["plt.figure(figsize=(10, 6))\n","plt.plot(subset_sizes, r2_results_tpeak_mean, marker='o')\n","plt.xlabel(\"Number of synthetic breakthrough curves\")\n","plt.ylabel(\"R$^2$ for time to peak\")\n","plt.grid(True)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o_TiCmGORVJP"},"outputs":[],"source":["plt.figure(figsize=(10, 6))\n","plt.plot(subset_sizes, mape_results_tpeak, marker='o')\n","plt.xlabel(\"Number of synthetic breakthrough curves\")\n","plt.ylabel(\"MAPE for time to peak (%)\")\n","plt.grid(True)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3bzBcTPHSN92"},"outputs":[],"source":["plt.figure(figsize=(10, 6))\n","plt.plot(subset_sizes, bias_results_tpeak, marker='o')\n","plt.xlabel(\"Number of synthetic breakthrough curves\")\n","plt.ylabel(\"Bias ratio for time to peak\")\n","plt.grid(True)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Spo4VupEP3Ho"},"outputs":[],"source":["plt.figure(figsize=(10, 6))\n","plt.plot(subset_sizes, r2_results_m0_mean, marker='o')\n","plt.xlabel(\"Number of synthetic breakthrough curves\")\n","plt.ylabel(\"R$^2$ for M0 \")\n","plt.grid(True)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zQPwIqvRRdCG"},"outputs":[],"source":["plt.figure(figsize=(10, 6))\n","plt.plot(subset_sizes, mape_results_m0, marker='o')\n","plt.xlabel(\"Number of synthetic breakthrough curves\")\n","plt.ylabel(\"MAPE for M0 (%) \")\n","plt.grid(True)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hy98hZ8VSY6C"},"outputs":[],"source":["plt.figure(figsize=(10, 6))\n","plt.plot(subset_sizes, bias_results_m0, marker='o')\n","plt.xlabel(\"Number of synthetic breakthrough curves\")\n","plt.ylabel(\"Bias ratio for M0\")\n","plt.grid(True)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QcE12TaYRjkv"},"outputs":[],"source":["plt.figure(figsize=(10, 6))\n","plt.plot(subset_sizes, r2_results_m1_mean, marker='o')\n","plt.xlabel(\"Number of synthetic breakthrough curves\")\n","plt.ylabel(\"R$^2$ for M1 \")\n","plt.grid(True)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0tuFx0UpR5x6"},"outputs":[],"source":["plt.figure(figsize=(10, 6))\n","plt.plot(subset_sizes, mape_results_m1, marker='o')\n","plt.xlabel(\"Number of synthetic breakthrough curves\")\n","plt.ylabel(\"MAPE for M1 (%) \")\n","plt.grid(True)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WKj-RVq2SbMu"},"outputs":[],"source":["plt.figure(figsize=(10, 6))\n","plt.plot(subset_sizes, bias_results_m1, marker='o')\n","plt.xlabel(\"Number of synthetic breakthrough curves\")\n","plt.ylabel(\"Bias ratio for M1 \")\n","plt.grid(True)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hgs4aYRp0SCd"},"outputs":[],"source":["x_values = subset_sizes\n","\n","val_loss = final_losses_mean\n","cross_corr = cc_results_mean\n","\n","mape_results_peak = mape_results_peak\n","mape_results_tpeak = mape_results_tpeak\n","mape_results_m0 = mape_results_m0\n","mape_results_m1 = mape_results_m1\n","\n","fig, axes = plt.subplots(1, 3, figsize=(20, 5), sharey=False)\n","\n","# (a) Validation loss\n","axes[0].plot(x_values, val_loss, marker='o', color='#1f77b4')\n","axes[0].set_xlabel(\"Number of synthetic breakthrough curves\")\n","axes[0].set_ylabel(\"Validation Loss (Final epoch)\")\n","axes[0].grid(True)\n","axes[0].set_title(\"(a)\", loc='left', fontsize=13, fontweight='bold')\n","\n","# (b) Cross-correlation\n","axes[1].plot(x_values, cross_corr, marker='s', color='#ff7f0e')\n","axes[1].set_xlabel(\"Number of synthetic breakthrough curves\")\n","axes[1].set_ylabel(\"Cross-Correlation Score\")\n","axes[1].grid(True)\n","axes[1].set_title(\"(b)\", loc='left', fontsize=13, fontweight='bold')\n","\n","# (c) MAPE metrics\n","axes[2].plot(x_values, mape_results_peak, marker='o', label=\"Peak\")\n","axes[2].plot(x_values, mape_results_tpeak, marker='s', label=\"Time to Peak\")\n","axes[2].plot(x_values, mape_results_m0, marker='^', label=r\"$M_0$\")\n","axes[2].plot(x_values, mape_results_m1, marker='d', label=r\"$M_1$\")\n","axes[2].set_xlabel(\"Number of synthetic breakthrough curves\")\n","axes[2].set_ylabel(\"MAPE (%)\")\n","axes[2].legend()\n","axes[2].grid(True)\n","axes[2].set_title(\"(c)\", loc='left', fontsize=13, fontweight='bold')\n","\n","plt.tight_layout(w_pad=3.5)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ty-ocRnikbgq"},"outputs":[],"source":["\n","save_path = \"/content/drive/MyDrive/\"  # Update to your local path\n","os.makedirs(save_path, exist_ok=True)\n","\n","df = pd.DataFrame({\n","    \"final_losses_mean\": final_losses_mean,\n","    \"train_losses_mean\": train_losses_mean,\n","    \"r2_results_mean\": r2_results_mean,\n","    \"r2_results_peak_mean\": r2_results_peak_mean,\n","    \"r2_results_tpeak_mean\": r2_results_tpeak_mean,\n","    \"r2_results_m0_mean\": r2_results_m0_mean,\n","    \"r2_results_m1_mean\": r2_results_m1_mean,\n","    \"cc_results_mean\": cc_results_mean,\n","    \"rmse_results_mean\": rmse_results_mean,\n","    \"mape_results_peak\": mape_results_peak,\n","    \"mape_results_tpeak\": mape_results_tpeak,\n","    \"mape_results_m0\": mape_results_m0,\n","    \"mape_results_m1\": mape_results_m1,\n","    \"bias_results_peak\": bias_results_peak,\n","    \"bias_results_tpeak\": bias_results_tpeak,\n","    \"bias_results_m0\": bias_results_m0,\n","})\n","\n","# 4. Save CSV in the specified path\n","csv_file = os.path.join(save_path, \"results_quantitymlp.csv\")\n","df.to_csv(csv_file, index=False)\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","provenance":[{"file_id":"1cK1pXCP0_-vNeS9YsHN2x6Tg23JUKu9n","timestamp":1765899636681}],"mount_file_id":"1cK1pXCP0_-vNeS9YsHN2x6Tg23JUKu9n","authorship_tag":"ABX9TyPf/WzxUtfl5nsj4DimnGDR"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}